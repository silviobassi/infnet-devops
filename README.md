# Projeto do Curso de P√≥s-gradua√ß√£o - MIT Arquitetura de Software

## Disciplina: Integra√ß√£o Cont√≠nua, DevOps e Computa√ß√£o em Nuvem [25E1_3]

Este projeto, implementa uma aplica√ß√£o com CI/CD via GitHub Actions, utilizando Docker e Kubernetes (Minikube). O ambiente inclui uma aplica√ß√£o com m√∫ltiplas r√©plicas, banco de dados MySQL e monitoramento completo com Prometheus e Grafana. A infraestrutura conta com readiness/liveness probes, servi√ßos LoadBalancer e persist√™ncia de dados com PVC. O projeto tamb√©m realiza testes de estresse com k6 e disponibiliza os dashboards para an√°lise de m√©tricas como uso de CPU e mem√≥ria.

Acesso ao reposit√≥rio em:
[https://github.com/silviobassi/infnet-devops](https://github.com/silviobassi/infnet-devops.git)

## 1. C√≥digo Fonte com Configura√ß√µes do Projeto

### 1.1. Pipeline de CI/CD com GitHub Actions - Acesse em üëá

- [.github/workflows/deploy.yml](https://github.com/silviobassi/infnet-devops/blob/main/.github/workflows/deploy.yml)

### 1.2. Manifestos e Configura√ß√µes da Aplica√ß√£o - Acesse em üëá

- [infra/manifests/k8s/app-deployment.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/k8s/app-deployment.yml)<br>
- [infra/manifests/k8s/app-service.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/k8s/app-service.yml)<br>
- [infra/manifests/k8s/mysql-config.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/k8s/mysql-config.yml)<br>
- [infra/manifests/k8s/mysql-deployment.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/k8s/mysql-deployment.yml)<br>
- [infra/manifests/k8s/mysql-service.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/k8s/mysql-service.yml)<br>

### 1.3. Manifestos e Configura√ß√µes do Grafana e Prometheus - Acesse em üëá

- [infra/manifests/monitoring/monitoring_install.sh](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/monitoring/monitoring_install.sh)<br>
- [infra/manifests/monitoring/prometheus-config.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/monitoring/prometheus-config.yml)<br>
- [infra/manifests/monitoring/values.yml](https://github.com/silviobassi/infnet-devops/tree/main/infra/manifests/monitoring/values.yml)<br>

### 1.4. Testes de Estresse - Acesse em üëá

- [stress-test/test_k6js](https://github.com/silviobassi/infnet-devops/blob/main/stress-test/test_k6.js)

## 2. Execu√ß√£o do Projeto

### 2.1. Instala√ß√£o do Docker, Kubectl e Minikube (Se n√£o estiverem instalados):

- [Instala√ß√£o do Docker](https://docs.docker.com/engine/install/)
- [Instala√ß√£o do Kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
- [Instala√ß√£o do Minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)

### 2.2 Execu√ß√£o do Deploy do Projeto:

- Clone o reposit√≥rio em sua m√°quina:

```bash
git clone https://github.com/silviobassi/infnet-devops.git
```

- ‚ö†Ô∏è No diret√≥rio do projeto, execute os comandos üëá

```bash
# Executa o deploy da aplica√ß√£o (App e MySQL)
minikube start --driver=docker 
cd infra/manifests/k8s
kubectl apply -f app-namespace.yml
kubectl apply -f .

# Executa o deploy do monitoramento (Grafana e Prometheus)
cd ../monitoring
sh monitoring_install.sh
```

## 3. Demonstra√ß√£o do Projeto em Execu√ß√£o:

### 3.1. Pipeline de CI/CD com GitHub Actions em Execu√ß√£o:

- Pipeline de gera√ß√£o da imagem da aplica√ß√£o

![Pipeline](devops-validate/workflow_pipeline_in_action.png)

- Imagem publicada no Docker Hub ap√≥s a execu√ß√£o do pipeline

![DockerHub](devops-validate/image_in_dockerhub.png)

### 3.2. Aplica√ß√£o em Execu√ß√£o:

- Deployment aplicado e exibi√ß√£o de pods, services e url para acesso a API da aplica√ß√£o

![App](devops-validate/app_in_execution_terminal.png)

- Swagger para acesso a API da aplica√ß√£o

![App](devops-validate/app_in_execution_browser.png)

### 3.3. Estrutura de Monitoramento da  Aplica√ß√£o com o Prometheus e o Grafana:

- Deployment aplicado

![Grafana](devops-validate/applied_deploy_monitoring.png)

- Exibi√ß√£o de pods, services e pvc do monitoramento

![Grafana](devops-validate/monitoring_pod_svc_pvc_in_action.png)

- O Prometheus s√≥ est√° exposto internamente _(ClusterIP)_, o acesso foi feito via _port-forward_

![Prometheus](devops-validate/prometheus_port_forward_terminal.png)

- Exibi√ß√£o do Prometheus em Execu√ß√£o

![Prometheus](devops-validate/prometheus_in_action.png)

- Grafana conectado ao prometheus

![Grafana](devops-validate/grafana_connected_to_prometheus.png)

### 3.4. Execu√ß√£o de _Stress Test_ com o k6

- Teste em progresso

![Grafana](devops-validate/test_in_progress_terminal.png)

- Resultado final

![Grafana](devops-validate/test_result_terminal.png)

- Houve _down_ ou _up_ para todos os pods, na progress√£o do teste. Os _POD(s)_ foram reiniciados em at√© 16 vezes, como se observa na figura abaixo:

![test](devops-validate/status_pods.png)

### 3.5. Resumo Geral do Teste

- Execu√ß√£o local
- Script: [stress-test/test_k6js](https://github.com/silviobassi/infnet-devops/blob/main/stress-test/test_k6.js)
- Cen√°rios (scenarios): 6 fases diferentes, com at√© 1500 VUs ativos ao longo do tempo.
- Dura√ß√£o total: 12m30s, incluindo gracefulStop (tempo de toler√¢ncia para finalizar VUs ativos).
- Total de requisi√ß√µes HTTP: 4402
- Total de checagens (check) realizadas: 8804 (duas por requisi√ß√£o, provavelmente)

#### 3.5.1. Configura√ß√£o das Fases do Teste

| Fase  | VUs (Usu√°rios Virtuais) | Itera√ß√µes por VU | In√≠cio | Dura√ß√£o M√°xima (minutos) |
|-------|-------------------------|------------------|--------|--------------------------|
| 1     | 100                     | 2                | 0s     | 2m                       |
| 2     | 500                     | 2                | 2m     | 3m                       |
| 3     | 1000                    | 2                | 5m     | 2m                       |
| 4     | 500                     | 2                | 7m     | 2m                       |
| 5     | 100                     | 2                | 9m     | 2m                       |
| Final | 1                       | 2                | 11m    | 1m                       |

- _Observa√ß√£o_: ‚ö†Ô∏è A fase final √© apenas simb√≥lica ‚Äî n√£o contribui de forma significativa a carga

#### 3.5.2. Checks (Valida√ß√µes do Teste)

| Check         | Sucesso | Falha  | Observa√ß√µes                                              |
|---------------|---------|--------|----------------------------------------------------------|
| status √© 200  | ‚úÖ 4731  | ‚ùå 0    | Bom! Todas as respostas vieram com status 200.           |
| tempo < 500ms | ‚úÖ 329   | ‚ùå 4073 | üò¨ Apenas 7% das requisi√ß√µes foram r√°pidas o suficiente. |

#### 3.5.3. Desempenho HTTP

| M√©trica        | Valor   |
|----------------|---------|
| M√©dia          | 4.48s   |
| Mediana (p50)  | 3.88s   |
| p90            | 8.44s   |
| p95            | 10.18s  |
| M√°ximo         | 15.9s   |
| M√≠nimo         | 6.27ms  |


#### 3.5.4. Execu√ß√£o de Itera√ß√µes e VUs

| M√©trica               | Valor     |
|-----------------------|-----------|
| Total de Itera√ß√µes    | 4402      |
| Itera√ß√µes por segundo | 6.87/s    |
| Dura√ß√£o m√©dia         | 4.59s     |
| Mediana (p50)         | 3.98s     |
| p90                   | 8.96s     |
| p95                   | 10.31s    |
| Dura√ß√£o m√°xima        | 15.97s    |
| Dura√ß√£o m√≠nima        | 6.41ms    |
| VUs em uso (m√≠n-m√°x)  | 0 - 1000  |
| VUs m√°ximos definidos | 1500      |

#### 3.5.5. M√©tricas de Rede

| M√©trica          | Valor     |
|------------------|-----------|
| Dados recebidos  | 1.7 MB    |
| Taxa de download | ~2.7 kB/s |
| Dados enviados   | 453 KB    |
| Taxa de upload   | ~708 B/s  |

#### 3.5.6. Resumo Final de Execu√ß√£o

| M√©trica                   | Valor        |
|---------------------------|--------------|
| Dura√ß√£o total da execu√ß√£o | 10m40.3s     |
| VUs ativos (m√≠n-m√°x)      | 0 - 1000     |
| VUs m√°ximos configurados  | 1500         |
| Total de requisi√ß√µes HTTP | 4402         |
| Falhas HTTP               | 0 (0.00%)    |

### 3.6. Dashboards do Grafana Expondo Dados Sens√≠veis dos _POD(s)_ da Aplica√ß√£o  - _Sofrendo Altera√ß√µes durante o _Stress Test_..._

- Consumo de mem√≥ria para todos os _POD(s)_ - in√≠cio

![Grafana](devops-validate/dashboard_memory_init.png)

- Consumo de cpu para todos os _POD(s)_ - in√≠cio

![Grafana](devops-validate/dashboard_cpu_init.png)

- Consumo de CPU do _POD_ com MySQL

![Grafana](devops-validate/dashboard_cpu_pod_mysql.png)

- Consumo de mem√≥ria do _POD_ com MySQL

![Grafana](devops-validate/dashboard_memory_pod_mysql.png)

- Consumo de CPU em um _POD_ com a aplica√ß√£o

![Grafana](devops-validate/dashboard_cpu_pod_app.png)

- Consumo de mem√≥ria em um _POD_ com a aplica√ß√£o

![Grafana](devops-validate/dashboard_memory_pod_app.png)

- Consumo de CPU pata todos os pods - sofrendo as √∫ltimas altera√ß√µes

![Grafana](devops-validate/dashboard_cpu_end_changing.png)

- Consumo de mem√≥ria para todos os pods - sofrendo as √∫ltimas altera√ß√µes

![Grafana](devops-validate/dashboard_memory_end_changing.png)

## Conclus√£o

‚úÖ Pontos Fortes

- Nenhuma falha HTTP detectada (100% das respostas com status 200).
- Estrutura de CI/CD bem definida com GitHub Actions e automa√ß√£o via Minikube.
- Monitoramento completo com Prometheus e Grafana em funcionamento.
- Execu√ß√£o est√°vel da aplica√ß√£o com at√© 1000 usu√°rios simult√¢neos.

‚ö†Ô∏è Pontos Fracos

- Apenas 7% das requisi√ß√µes foram conclu√≠das abaixo de 500ms.
- Lat√™ncia m√©dia elevada (~4.5s), com picos de at√© 15.9s.
- Consumo elevado de CPU e mem√≥ria nos pods da aplica√ß√£o e do MySQL.
- Reinicializa√ß√µes excessivas dos pods durante o teste de estresse.

üõ†Ô∏è Recomenda√ß√µes para Melhoria

- Otimizar o backend e as consultas ao banco de dados.
- Implementar cache estrat√©gico para dados mais acessados.
- Avaliar e ajustar os limites de CPU e mem√≥ria dos nodes no Kubernetes.
- Ativar o autoescalonamento horizontal (HPA) baseado em m√©tricas de uso.
- Revisar as configura√ß√µes de readiness/liveness probes para evitar rein√≠cios desnecess√°rios.
- Verificar se o cluster possui n√≥s suficientes para suportar a carga simulada.